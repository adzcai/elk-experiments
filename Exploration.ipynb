{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliciting latent knowledge - experiments\n",
    "\n",
    "Alexander Cai, Gabriel Wu, Max Nadeau\n",
    "\n",
    "Some experiments initially performed for [CS 229br Foundations of Deep Learning](https://boazbk.github.io/mltheoryseminar/) as taught in Spring 2023 at Harvard University by Boaz Barak.\n",
    "\n",
    "This is a work-in-progress research draft to explore some properties of the \"Contrast-Consistent Search\" algorithm (and later algorithms)\n",
    "for identifying a language model's internal representation of truth. This would be helpful in identifying model misbehaviours\n",
    "or \"eliciting latent knowledge\" from intelligent models.\n",
    "\n",
    "Our research questions:\n",
    "\n",
    "- Does the \"direction\" discovered by CCS carry any semantic meaning outside its original setting (i.e. the residual stream on the final \"positive\" / \"negative\" token)?\n",
    "\n",
    "See [adzcai/llama-ccs](https://github.com/adzcai/llama-ccs) for some preliminary experiments on the Meta LLaMA models.\n",
    "\n",
    "Currently the [EleutherAI/elk](https://github.com/EleutherAI/elk) package must be installed in editable mode to download the associated prompt templates. Also note that it requires Python 3.10 which is not supported on Google Colab.\n",
    "\n",
    "To do this, make sure you have the desired environment enabled, navigate to a convenient repository, and run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/EleutherAI/elk.git\n",
    "# ! cd elk && pip install -qe ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the remaining requirements\n",
    "\n",
    "# ! pip install -q \\\n",
    "#     circuitsvis \\\n",
    "#     plotly \\\n",
    "#     git+https://github.com/neelnanda-io/TransformerLens.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "[EleutherAI/elk](https://github.com/EleutherAI/elk): Contains many further innovations on top of CCS. Very convenient tool for interacting with HF models and datasets.\n",
    "\n",
    "[Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827): The original paper by Collin Burns and Haotian Ye et al that proposes \"Contrast-Consistent Search\" (CCS).\n",
    "- [collin-burns/discovering_latent_knowledge](https://github.com/collin-burns/discovering_latent_knowledge): The corresponding repository.\n",
    "  - This is claimed to be quite buggy. See [Bugs of the Initial Release of CCS](https://docs.google.com/document/d/16Q8ZJFloA-x2lR65hs80rbbjX70TteCSMhuDQGcC75Q/edit?usp=sharing) by Fabien Roger.\n",
    "- [How \"Discovering Latent Knowledge in Language Models Without Supervision\" Fits Into a Broader Alignment Scheme](https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without)\n",
    "\n",
    "[What Discovering Latent Knowledge Did and Did Not Find](https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4): A writeup by Fabien Roger on takeaways from the original paper.\n",
    "\n",
    "- [safer-ai/Exhaustive-CCS](https://github.com/safer-ai/Exhaustive-CCS): The corresponding repository. Similar to Collin Burns's but with fewer bugs.\n",
    "- [Several experiments with CCS.](https://docs.google.com/document/d/1LCjjnUPN51gHl_rmCWEmmtbY-Wu1dixzOif14e-7i-U/edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path(os.getcwd())\n",
    "data_path = cwd / \"data\"\n",
    "reporters_path = cwd / \"reporters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data_dir = True\n",
    "\"\"\"Optionally store data in this folder instead of the default.\"\"\"\n",
    "\n",
    "if use_data_dir:\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    os.environ[\"HF_HOME\"] = data_path.as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we elicit latent knowledge from the [Pythia](https://github.com/EleutherAI/pythia) model family from EleutherAI.\n",
    "We used the _non-deduplicated_ version of the models as of 17 April 2023. We use the 1B and 1.4B parameter models.\n",
    "\n",
    "This model are notable in that every model in the family is trained on the same data in the same order.\n",
    "A [paper](https://arxiv.org/pdf/2304.01373.pdf) with detailed information about these models is also available.\n",
    "\n",
    "Additionally, these models are also available for use with [TransformerLens](https://github.com/neelnanda-io/TransformerLens/blob/main/transformer_lens/model_properties_table.md).\n",
    "\n",
    "We use the [SuperGLUE (BoolQ)](https://huggingface.co/datasets/super_glue/viewer/boolq/test) dataset for a QA task and the [IMDB](https://huggingface.co/datasets/imdb) dataset for sentiment analysis.\n",
    "\n",
    "We chose these datasets for preliminary analysis since they're simple archetypes for their respective tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! elk elicit EleutherAI/pythia-1b 'super_glue boolq' --net ccs --out_dir 'reporters/ccs/pythia-1b/super_glue boolq'\n",
    "\n",
    "# ! elk elicit EleutherAI/pythia-1b 'super_glue boolq' --net eigen --out_dir 'reporters/eigen/pythia-1b/super_glue boolq'\n",
    "\n",
    "# ! elk elicit EleutherAI/pythia-1b imdb --net ccs --out_dir reporters/ccs/pythia-1b/imdb\n",
    "\n",
    "# ! elk elicit EleutherAI/pythia-1b imdb --net eigen --out_dir reporters/eigen/pythia-1b/imdb\n",
    "\n",
    "# ! elk elicit EleutherAI/pythia-1.4b 'super_glue boolq' --net ccs --out_dir reporters/ccs/pythia-1.4b/'super_glue boolq'\n",
    "\n",
    "# ! elk elicit EleutherAI/pythia-1.4b 'super_glue boolq' --net eigen --out_dir reporters/eigen/pythia-1.4b/'super_glue boolq'\n",
    "\n",
    "# ! elk elicit EleutherAI/pythia-1.4b imdb --net ccs --out_dir reporters/ccs/pythia-1.4b/imdb\n",
    "\n",
    "# ! elk elicit EleutherAI/pythia-1.4b imdb --net eigen --out_dir reporters/eigen/pythia-1.4b/imdb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the learned directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# disable gradients since we're not doing any training here\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_path = reporters_path / \"ccs/pythia-1b/imdb\"\n",
    "list(ccs_path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporters = [\n",
    "    torch.load(reporter, map_location=device)\n",
    "    for reporter in (ccs_path / \"reporters\").iterdir()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "We use the [TransformerLens](https://github.com/neelnanda-io/TransformerLens) library to interact with model internals.\n",
    "\n",
    "The reference documentation can be found [here](https://neelnanda-io.github.io/TransformerLens/transformer_lens.html).\n",
    "\n",
    "The [main tutorial](https://neelnanda.io/transformer-lens-demo) was very helpful in getting started with the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"EleutherAI/pythia-1b\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "n_layers, len(reporters) == n_layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prompts\n",
    "\n",
    "In the original DLK paper, the authors found that out of the models they tested,\n",
    "the single decoder-only model (GPT-J-6B) performed the worst (measured in terms of accuracy across datasets).\n",
    "\n",
    "Later on, researchers at EleutherAI found that this could be resolved by prompting the model using different prompt templates.\n",
    "Their method leverages that the truth of the given statement should be the same regardless of the prompt template chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elk.extraction.prompt_loading import load_prompts\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "n_prompts = 12\n",
    "\n",
    "prompt_dataset = load_prompts(\"imdb\", split_type=\"val\")\n",
    "prompt_dataset = list(itertools.islice(prompt_dataset, n_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded prompts (each element of the `prompt_dataset` have the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"label\": 0 if correct answer is \"negative\", 1 if correct answer is \"positive\"\n",
    "    \"prompts\": [\n",
    "        [\n",
    "            {\n",
    "                \"answer\": \"negative\" or \"bad\" or ...\n",
    "                \"text\": formatted prompt with the \"negative\" answer\n",
    "            },\n",
    "            {\n",
    "                \"answer\": \"positive\" or \"good\" or ...\n",
    "                \"text\": formatted prompt with the \"positive\" answer\n",
    "            }\n",
    "        ],\n",
    "        ...\n",
    "    ],\n",
    "    \"template_names\": [\n",
    "        template name for prompt 0,\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Note that prompts vary along two binary axes:\n",
    "\n",
    "1. Whether the statement ends in the \"positive\" answer or the \"negative\" answer;\n",
    "2. Whether the statement is factually correct or incorrect.\n",
    "\n",
    "It's important to distinguish these two axes; The goal of CCS is to uncover the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        dict(\n",
    "            negative_prompt=negative[\"text\"],\n",
    "            positive_prompt=positive[\"text\"],\n",
    "            negative_answer=negative[\"answer\"],\n",
    "            positive_answer=positive[\"answer\"],\n",
    "            incorrect_answer=negative[\"answer\"]\n",
    "            if prompts[\"label\"]\n",
    "            else positive[\"answer\"],\n",
    "            correct_answer=positive[\"answer\"]\n",
    "            if prompts[\"label\"]\n",
    "            else negative[\"answer\"],\n",
    "            template_name=template_name,\n",
    "            template_id=i,\n",
    "            prompt_id=j,\n",
    "        )\n",
    "        for j, prompts in enumerate(prompt_dataset)\n",
    "        for i, ((negative, positive), template_name) in enumerate(\n",
    "            zip(prompts[\"prompts\"], prompts[\"template_names\"])\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "# set a multiindex using the prompt_id and template_id\n",
    "df = df.set_index([\"prompt_id\", \"template_id\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.at[(0, 0), \"negative_prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "from tqdm import tqdm\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import transformer_lens.utils as utils\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fix a given template ID. This gives us a single fully-formatted prompt for each original text sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_id = 1\n",
    "prompts = df.loc[pd.IndexSlice[:, template_id], :].reset_index(drop=True)\n",
    "prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_prompts = prompts[\"negative_prompt\"].tolist()\n",
    "pos_prompts = prompts[\"positive_prompt\"].tolist()\n",
    "correct_answers = prompts[\"correct_answer\"].tolist()\n",
    "incorrect_answers = prompts[\"incorrect_answer\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tokens = model.to_tokens(neg_prompts)\n",
    "pos_tokens = model.to_tokens(pos_prompts)\n",
    "\n",
    "neg_str_tokens = model.to_str_tokens(neg_prompts)\n",
    "pos_str_tokens = model.to_str_tokens(pos_prompts)\n",
    "\n",
    "prompt_lengths = torch.tensor([len(tokens) for tokens in neg_str_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the negative and positive prompts match up until the very last token,\n",
    "# so we just record one and then get only the last token of the other one\n",
    "neg_results = torch.zeros((n_prompts, n_layers, max(prompt_lengths)), device=\"cpu\")\n",
    "pos_results = torch.zeros((n_prompts, n_layers), device=\"cpu\")\n",
    "batch_range = torch.arange(n_prompts)\n",
    "\n",
    "\n",
    "def projection(\n",
    "    resid_pre: Float[torch.Tensor, \"batch pos d_model\"],  # at a given layer\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    "):\n",
    "    # TODO should we be normalizing here?\n",
    "    # since the prompts have different lengths, technically this is more computation than we need to do\n",
    "    neg_results[:, layer, :] = reporters[layer](resid_pre).cpu()\n",
    "    return resid_pre\n",
    "\n",
    "\n",
    "def final_projection(\n",
    "    resid_pre: Float[torch.Tensor, \"batch pos d_model\"],  # at a given layer\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    "):\n",
    "    # TODO should we be normalizing here?\n",
    "    x = resid_pre[batch_range, prompt_lengths - 1, :]\n",
    "    pos_results[:, layer] = reporters[layer](x).cpu()\n",
    "    return resid_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be quite fast with a few gpus\n",
    "for layer in tqdm(range(n_layers)):\n",
    "    act_name = utils.get_act_name(\"resid_pre\", layer)\n",
    "\n",
    "    patch_hook_fn = partial(projection, layer=layer)\n",
    "    model.run_with_hooks(neg_tokens, fwd_hooks=[(act_name, patch_hook_fn)])\n",
    "\n",
    "    patch_hook_fn = partial(final_projection, layer=layer)\n",
    "    model.run_with_hooks(pos_tokens, fwd_hooks=[(act_name, patch_hook_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(f\"results/ccs/pythia-1b/imdb/template-{template_id}\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(neg_results, save_path / \"neg.pt\")\n",
    "torch.save(pos_results, save_path / \"pos.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "from circuitsvis.tokens import colored_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_results = torch.load(save_path / \"neg.pt\")\n",
    "pos_results = torch.load(save_path / \"pos.pt\")\n",
    "\n",
    "projections = torch.cat([neg_results, torch.zeros((n_prompts, n_layers, 2))], axis=-1)\n",
    "projections[batch_range, :, prompt_lengths] = pos_results\n",
    "projections[batch_range, :, prompt_lengths + 1] = 0  # visualization\n",
    "\n",
    "# make the signs across layers consistent with the final token\n",
    "# since CCS only identifies the hyperplane up to sign\n",
    "projections = pos_results.sign()[:, :, None] * projections\n",
    "projections.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose a given prompt to visualize. \"positive\" answers always corresponds to _blue_ and \"negative\" answers always correspond to _red_\n",
    "independently of the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_colors(prompt_id: int):\n",
    "    flattened_tokens = (\n",
    "        neg_str_tokens[prompt_id] + pos_str_tokens[prompt_id][-1:] + [\"\\n\\n\\n\"]\n",
    "    ) * n_layers\n",
    "    flattened_projections = projections[\n",
    "        prompt_id, :, : prompt_lengths[prompt_id] + 2\n",
    "    ].flatten()\n",
    "\n",
    "    # clip the values to between (-5.5, 4) to make the visualization more readable\n",
    "    print(\"distance bounds:\", flattened_projections.min(), flattened_projections.max())\n",
    "    return colored_tokens(\n",
    "        flattened_tokens, flattened_projections, min_value=-5, max_value=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors(prompt_id=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that these almost look like attention patterns! The blue tokens are often ones that carry some sort of positive connotation\n",
    "and the red ones often carry some kind of negative connotation. Let's see if there's any similarities with the actual attention patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(neg_tokens[4][:12])\n",
    "attention_pattern = cache['pattern', 0, 'attn']\n",
    "cv.attention.attention_patterns(tokens=neg_str_tokens[4][:12], attention=attention_pattern)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
